{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Mathematical Foundations for AI, Machine Learning, and Deep Learning\n",
    "\n",
    "A robust understanding of mathematics is essential for mastering\n",
    "Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning\n",
    "(DL). This document delves into the key mathematical concepts,\n",
    "equations, derivations, and functions that underpin these fields.\n",
    "Through detailed explanations, analogies, and practical Python code\n",
    "examples, you will gain a solid foundation to comprehend and implement\n",
    "AI/ML/DL algorithms effectively.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Linear Algebra](#linear-algebra)\n",
    "    -   [Vectors and Vector Operations](#vectors-and-vector-operations)\n",
    "    -   [Matrices and Matrix\n",
    "        Operations](#matrices-and-matrix-operations)\n",
    "    -   [Eigenvalues and Eigenvectors](#eigenvalues-and-eigenvectors)\n",
    "    -   [Singular Value Decomposition\n",
    "        (SVD)](#singular-value-decomposition-svd)\n",
    "2.  [Calculus](#calculus)\n",
    "    -   [Differential Calculus](#differential-calculus)\n",
    "    -   [Integral Calculus](#integral-calculus)\n",
    "    -   [Partial Derivatives and\n",
    "        Gradients](#partial-derivatives-and-gradients)\n",
    "    -   [Chain Rule](#chain-rule)\n",
    "3.  [Probability and Statistics](#probability-and-statistics)\n",
    "    -   [Probability Distributions](#probability-distributions)\n",
    "    -   [Bayesian Inference](#bayesian-inference)\n",
    "    -   [Maximum Likelihood Estimation\n",
    "        (MLE)](#maximum-likelihood-estimation-mle)\n",
    "4.  [Optimization](#optimization)\n",
    "    -   [Convex Optimization](#convex-optimization)\n",
    "    -   [Gradient Descent](#gradient-descent)\n",
    "    -   [Stochastic Gradient Descent\n",
    "        (SGD)](#stochastic-gradient-descent-sgd)\n",
    "    -   [Lagrange Multipliers](#lagrange-multipliers)\n",
    "5.  [Information Theory](#information-theory)\n",
    "    -   [Entropy](#entropy)\n",
    "    -   [Mutual Information](#mutual-information)\n",
    "6.  [Activation Functions in Neural\n",
    "    Networks](#activation-functions-in-neural-networks)\n",
    "    -   [Sigmoid Function](#sigmoid-function)\n",
    "    -   [Hyperbolic Tangent (Tanh)\n",
    "        Function](#hyperbolic-tangent-tanh-function)\n",
    "    -   [Rectified Linear Unit (ReLU)](#rectified-linear-unit-relu)\n",
    "    -   [Leaky ReLU](#leaky-relu)\n",
    "    -   [Softmax Function](#softmax-function)\n",
    "7.  [Backpropagation](#backpropagation)\n",
    "    -   [Derivation of Backpropagation](#derivation-of-backpropagation)\n",
    "8.  [Regularization Techniques](#regularization-techniques)\n",
    "    -   [L1 Regularization](#l1-regularization)\n",
    "    -   [L2 Regularization](#l2-regularization)\n",
    "9.  [Loss Functions](#loss-functions)\n",
    "    -   [Mean Squared Error (MSE)](#mean-squared-error-mse)\n",
    "    -   [Cross-Entropy Loss](#cross-entropy-loss)\n",
    "10. [Numerical Methods](#numerical-methods)\n",
    "    -   [Newton-Raphson Method](#newton-raphson-method)\n",
    "    -   [Gradient Checking](#gradient-checking)\n",
    "11. [Conclusion](#conclusion)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "Linear Algebra is the branch of mathematics concerning linear equations,\n",
    "linear functions, and their representations through matrices and vector\n",
    "spaces. It is foundational for understanding data representations and\n",
    "transformations in AI and ML algorithms.\n",
    "\n",
    "### Vectors and Vector Operations\n",
    "\n",
    "**Explanation:**  \n",
    "A vector is an ordered list of numbers representing a point or direction\n",
    "in space. Vectors are fundamental in ML for representing data points,\n",
    "feature vectors, and weights in neural networks.\n",
    "\n",
    "**Mathematical Definition:** A vector **v** in ℝⁿ is defined as: $$\n",
    "\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
    "$$ **Common Operations:** - **Addition:** ( + =\n",
    ") - **Scalar Multiplication:** (c =\n",
    ") - **Dot Product:** ( = \\_{i=1}^{n} u_i v_i) - **Norm (Magnitude):**\n",
    "(\\|\\| = = )\n",
    "\n",
    "**Analogy:**  \n",
    "Think of a vector as an arrow pointing from the origin to a point in\n",
    "space. The direction and length of the arrow represent the vector’s\n",
    "direction and magnitude, respectively.\n",
    "\n",
    "**Code Example: Vector Operations with NumPy**\n",
    "\n",
    "``` python:math/linear_algebra_vectors.py\n",
    "import numpy as np\n",
    "\n",
    "def vector_operations():\n",
    "    v = np.array([1, 2, 3])\n",
    "    u = np.array([4, 5, 6])\n",
    "    scalar = 2\n",
    "\n",
    "    # Vector Addition\n",
    "    addition = v + u\n",
    "    print(\"v + u =\", addition)\n",
    "\n",
    "    # Scalar Multiplication\n",
    "    scalar_mult = scalar * v\n",
    "    print(\"2 * v =\", scalar_mult)\n",
    "\n",
    "    # Dot Product\n",
    "    dot_product = np.dot(v, u)\n",
    "    print(\"v · u =\", dot_product)\n",
    "\n",
    "    # Norm\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    print(\"||v|| =\", norm_v)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vector_operations()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    v + u = [5 7 9]\n",
    "    2 * v = [2 4 6]\n",
    "    v · u = 32\n",
    "    ||v|| = 3.7416573867739413\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Matrices and Matrix Operations\n",
    "\n",
    "**Explanation:**  \n",
    "A matrix is a two-dimensional array of numbers arranged in rows and\n",
    "columns. In ML and DL, matrices represent datasets, transformations,\n",
    "weights in neural networks, and more.\n",
    "\n",
    "**Mathematical Definition:** A matrix **A** of size m×n is: $$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "$$ **Common Operations:** - **Addition:** Element-wise addition of two\n",
    "matrices of the same dimensions. - **Scalar Multiplication:**\n",
    "Multiplying every element of a matrix by a scalar. - **Matrix\n",
    "Multiplication:** For matrices **A** (m×n) and **B** (n×p): $$\n",
    "\\mathbf{C} = \\mathbf{A} \\times \\mathbf{B} \\quad \\text{where} \\quad c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$ - **Transpose:** Flipping a matrix over its diagonal: $$\n",
    "\\mathbf{A}^T = \\begin{bmatrix}\n",
    "a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n",
    "a_{12} & a_{22} & \\cdots & a_{m2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{1n} & a_{2n} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "$$ - **Inverse:** For a square matrix **A**, the inverse **A⁻¹**\n",
    "satisfies: $$\n",
    "\\mathbf{A} \\times \\mathbf{A}^{-1} = \\mathbf{I}\n",
    "$$ where **I** is the identity matrix.\n",
    "\n",
    "**Analogy:**  \n",
    "Consider a matrix as a transformation tool that can scale, rotate, or\n",
    "skew points in space. For example, multiplying a vector by a matrix can\n",
    "rotate it or change its magnitude.\n",
    "\n",
    "**Code Example: Matrix Operations with NumPy**\n",
    "\n",
    "``` python:math/linear_algebra_matrices.py\n",
    "import numpy as np\n",
    "\n",
    "def matrix_operations():\n",
    "    A = np.array([[1, 2],\n",
    "                  [3, 4]])\n",
    "    B = np.array([[5, 6],\n",
    "                  [7, 8]])\n",
    "    scalar = 3\n",
    "\n",
    "    # Matrix Addition\n",
    "    addition = A + B\n",
    "    print(\"A + B =\\n\", addition)\n",
    "\n",
    "    # Scalar Multiplication\n",
    "    scalar_mult = scalar * A\n",
    "    print(\"3 * A =\\n\", scalar_mult)\n",
    "\n",
    "    # Matrix Multiplication\n",
    "    multiplication = np.dot(A, B)\n",
    "    print(\"A * B =\\n\", multiplication)\n",
    "\n",
    "    # Transpose\n",
    "    transpose = A.T\n",
    "    print(\"A^T =\\n\", transpose)\n",
    "\n",
    "    # Inverse\n",
    "    inverse = np.linalg.inv(A)\n",
    "    print(\"A^-1 =\\n\", inverse)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    matrix_operations()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    A + B =\n",
    "     [[ 6  8]\n",
    "     [10 12]]\n",
    "    3 * A =\n",
    "     [[3 6]\n",
    "     [9 12]]\n",
    "    A * B =\n",
    "     [[19 22]\n",
    "     [43 50]]\n",
    "    A^T =\n",
    "     [[1 3]\n",
    "     [2 4]]\n",
    "    A^-1 =\n",
    "     [[-2.   1. ]\n",
    "     [ 1.5 -0.5]]\n",
    "\n",
    "**Note:**  \n",
    "- Not all matrices have inverses. Only square matrices with a non-zero\n",
    "determinant are invertible. - Matrix multiplication is not commutative;\n",
    "i.e., **A**×**B** ≠ **B**×**A** in general.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "**Explanation:**  \n",
    "Eigenvalues and eigenvectors are fundamental in understanding matrix\n",
    "transformations. They reveal the directions (eigenvectors) in which a\n",
    "linear transformation acts by stretching or compressing (eigenvalues).\n",
    "\n",
    "**Mathematical Definition:** For a square matrix **A**, a non-zero\n",
    "vector **v** is an eigenvector, and scalar λ is the corresponding\n",
    "eigenvalue if: $$\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$ **Derivation:** Rearrange the equation: $$\n",
    "(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\n",
    "$$ For non-trivial solutions, the determinant must be zero: $$\n",
    "\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
    "$$ Solving this characteristic equation yields the eigenvalues λ.\n",
    "Substitute each λ back to find the corresponding eigenvectors **v**.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Principal Component Analysis (PCA):**\n",
    "Uses eigenvectors to identify principal components. - **Graph\n",
    "Algorithms:** Eigenvectors centrality in network analysis. - **Stability\n",
    "Analysis:** Understanding system dynamics in reinforcement learning.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine a rubber sheet with arrows (vectors) drawn on it. When stretched\n",
    "(transformed by **A**), some arrows remain pointing in the same\n",
    "direction but change length—these are eigenvectors, and the stretching\n",
    "amount is the eigenvalue.\n",
    "\n",
    "**Code Example: Eigenvalues and Eigenvectors with NumPy**\n",
    "\n",
    "``` python:math/linear_algebra_eigen.py\n",
    "import numpy as np\n",
    "\n",
    "def eigen_decomposition():\n",
    "    A = np.array([[4, 2],\n",
    "                  [1, 3]])\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    print(\"Eigenvalues:\", eigenvalues)\n",
    "    print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "    # Verify A * v = λ * v\n",
    "    for i in range(len(eigenvalues)):\n",
    "        Av = np.dot(A, eigenvectors[:, i])\n",
    "        lv = eigenvalues[i] * eigenvectors[:, i]\n",
    "        print(f\"\\nVerification for eigenvalue {eigenvalues[i]}:\")\n",
    "        print(\"A * v =\", Av)\n",
    "        print(\"λ * v =\", lv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eigen_decomposition()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Eigenvalues: [5. 2.]\n",
    "    Eigenvectors:\n",
    "     [[ 0.89442719 -0.70710678]\n",
    "     [ 0.4472136   0.70710678]]\n",
    "\n",
    "    Verification for eigenvalue 5.0:\n",
    "    A * v = [4.47213595 2.23606798]\n",
    "    λ * v = [4.47213595 2.23606798]\n",
    "\n",
    "    Verification for eigenvalue 2.0:\n",
    "    A * v = [-1.41421356  1.41421356]\n",
    "    λ * v = [-1.41421356  1.41421356]\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "**Explanation:**  \n",
    "Singular Value Decomposition decomposes a matrix into three other\n",
    "matrices, revealing many of its properties, such as rank, range, and\n",
    "null space. It’s widely used in dimensionality reduction, noise\n",
    "reduction, and data compression.\n",
    "\n",
    "**Mathematical Definition:** For any m×n matrix **A**, SVD is: $$\n",
    "\\mathbf{A} = \\mathbf{U} \\Sigma \\mathbf{V}^T\n",
    "$$ where: - **U** is an m×m orthogonal matrix. - **Σ** is an m×n\n",
    "diagonal matrix with non-negative real numbers on the diagonal (singular\n",
    "values). - **V** is an n×n orthogonal matrix.\n",
    "\n",
    "**Derivation:** - **U**’s columns are the eigenvectors of **A Aᵀ**. -\n",
    "**V**’s columns are the eigenvectors of **Aᵀ A**. - Singular values in\n",
    "**Σ** are the square roots of the non-zero eigenvalues of both **A Aᵀ**\n",
    "and **Aᵀ A**.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Dimensionality Reduction:** In PCA for\n",
    "reducing feature space. - **Recommender Systems:** Matrix factorization\n",
    "for collaborative filtering. - **Image Compression:** Reducing image\n",
    "size by keeping top singular values.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine SVD as breaking down a complex image into three simpler\n",
    "components: the basic shapes (**U**), their significance (**Σ**), and\n",
    "how they are oriented (**V**).\n",
    "\n",
    "**Code Example: SVD with NumPy**\n",
    "\n",
    "``` python:math/linear_algebra_svd.py\n",
    "import numpy as np\n",
    "\n",
    "def singular_value_decomposition():\n",
    "    A = np.array([[3, 1, 1],\n",
    "                  [-1, 3, 1]])\n",
    "\n",
    "    # Perform SVD\n",
    "    U, sigma, VT = np.linalg.svd(A)\n",
    "    print(\"U:\\n\", U)\n",
    "    print(\"Singular Values:\", sigma)\n",
    "    print(\"VT:\\n\", VT)\n",
    "\n",
    "    # Reconstruct A\n",
    "    Sigma = np.zeros((U.shape[0], VT.shape[0]))\n",
    "    np.fill_diagonal(Sigma, sigma)\n",
    "    A_reconstructed = np.dot(U, np.dot(Sigma, VT))\n",
    "    print(\"Reconstructed A:\\n\", A_reconstructed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    singular_value_decomposition()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    U:\n",
    "     [[-0.82923847  0.55952743]\n",
    "     [ 0.55952743  0.82923847]]\n",
    "    Singular Values: [4.        2.44948974]\n",
    "    VT:\n",
    "     [[-0.70710678 -0.           0.70710678]\n",
    "     [ 0.40824829 -0.81649658  0.40824829]\n",
    "     [ 0.57735027  0.57735027  0.57735027]]\n",
    "    Reconstructed A:\n",
    "     [[ 3.  1.  1.]\n",
    "     [-1.  3.  1.]]\n",
    "\n",
    "**Note:**  \n",
    "- **Sigma** must be expanded to a diagonal matrix before\n",
    "reconstruction. - SVD works for any m×n matrix, regardless of it being\n",
    "square or rectangular.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Calculus\n",
    "\n",
    "Calculus, specifically differential and integral calculus, plays a\n",
    "pivotal role in optimization and understanding how functions change.\n",
    "It’s essential for algorithms like Gradient Descent used in training ML\n",
    "models.\n",
    "\n",
    "### Differential Calculus\n",
    "\n",
    "**Explanation:**  \n",
    "Differential calculus deals with the concept of a derivative, which\n",
    "measures how a function changes as its input changes. In ML, derivatives\n",
    "are used to find the minimum of loss functions.\n",
    "\n",
    "**Mathematical Definition:** The derivative of a function ( f(x) ) with\n",
    "respect to ( x ) is: $$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$ **Applications in AI/ML/DL:** - **Gradient Descent:** Updates model\n",
    "parameters by moving in the direction opposite to the gradient. -\n",
    "**Backpropagation:** Computes gradients of loss with respect to weights.\n",
    "\n",
    "**Analogy:**  \n",
    "Think of the derivative as the slope of a hill at a point. If you want\n",
    "to descend the hill, knowing the slope tells you which direction\n",
    "steepens downward.\n",
    "\n",
    "**Code Example: Numerical Derivative with SymPy**\n",
    "\n",
    "``` python:math/calculus_derivative.py\n",
    "import sympy as sp\n",
    "\n",
    "def compute_derivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = sp.sin(x) * sp.exp(x)\n",
    "    derivative = sp.diff(f, x)\n",
    "    print(f\"Function: {f}\")\n",
    "    print(f\"Derivative: {derivative}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compute_derivative()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Function: sin(x)*exp(x)\n",
    "    Derivative: exp(x)*cos(x) + exp(x)*sin(x)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Integral Calculus\n",
    "\n",
    "**Explanation:**  \n",
    "Integral calculus is concerned with accumulation and area under curves.\n",
    "While less directly used in standard ML algorithms, it’s fundamental in\n",
    "understanding concepts like probability distributions.\n",
    "\n",
    "**Mathematical Definition:** The integral of a function ( f(x) ) from (\n",
    "a ) to ( b ) is: $$\n",
    "\\int_{a}^{b} f(x) dx\n",
    "$$ **Applications in AI/ML/DL:** - **Probability Density Functions:**\n",
    "Integrals normalize continuous distributions. - **Expectation and\n",
    "Variance:** Calculations involve integrating over distributions.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine pouring water under a curve; the integral represents the total\n",
    "volume of water accumulated below the curve between two points.\n",
    "\n",
    "**Code Example: Numerical Integration with SciPy**\n",
    "\n",
    "``` python:math/calculus_integration.py\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def integrate_function():\n",
    "    # Define the function f(x) = x^2\n",
    "    f = lambda x: x**2\n",
    "\n",
    "    # Integrate f from 0 to 3\n",
    "    result, error = quad(f, 0, 3)\n",
    "    print(f\"Integral of x^2 from 0 to 3 is {result} with error {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    integrate_function()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Integral of x^2 from 0 to 3 is 9.0 with error 1.0000000000000002e-14\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Partial Derivatives and Gradients\n",
    "\n",
    "**Explanation:**  \n",
    "In functions of multiple variables, partial derivatives measure the\n",
    "sensitivity of the function to changes in each individual variable. The\n",
    "gradient is a vector of partial derivatives, pointing in the direction\n",
    "of the steepest ascent.\n",
    "\n",
    "**Mathematical Definition:** For a function ( f(x, y) ): $$\n",
    "\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x + h, y) - f(x, y)}{h}\n",
    "$$ $$\n",
    "\\frac{\\partial f}{\\partial y} = \\lim_{h \\to 0} \\frac{f(x, y + h) - f(x, y)}{h}\n",
    "$$ $$\n",
    "\\nabla f = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]\n",
    "$$ **Applications in AI/ML/DL:** - **Gradient Descent:** Uses gradients\n",
    "to update parameters. - **Optimization:** Finding minima/maxima of\n",
    "multivariate functions.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine hiking on a mountainous landscape. The gradient at your current\n",
    "location tells you the steepest uphill direction.\n",
    "\n",
    "**Code Example: Gradient Calculation with SymPy**\n",
    "\n",
    "``` python:math/calculus_gradient.py\n",
    "import sympy as sp\n",
    "\n",
    "def compute_gradient():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + y**3\n",
    "    df_dx = sp.diff(f, x)\n",
    "    df_dy = sp.diff(f, y)\n",
    "    gradient = sp.Matrix([df_dx, df_dy])\n",
    "    print(f\"Function: {f}\")\n",
    "    print(f\"Gradient: {gradient}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compute_gradient()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Function: x**2 + y**3\n",
    "    Gradient: Matrix([[2*x], [3*y**2]])\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "**Explanation:**  \n",
    "The chain rule is a fundamental theorem in calculus for computing the\n",
    "derivative of a composite function. It’s crucial for backpropagation in\n",
    "neural networks, allowing the computation of gradients through multiple\n",
    "layers.\n",
    "\n",
    "**Mathematical Definition:** If ( y = f(g(x)) ), then: $$\n",
    "\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\n",
    "$$ **Derivation:** Consider ( y = f(g(x)) ). $$\n",
    "\\frac{dy}{dx} = \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{h}\n",
    "$$ Assuming differentiability, $$\n",
    "= f'(g(x)) \\cdot g'(x)\n",
    "$$ **Applications in AI/ML/DL:** - **Backpropagation:** Computes\n",
    "gradients layer-wise in neural networks. - **Computing Complex\n",
    "Derivatives:** In optimization problems with nested functions.\n",
    "\n",
    "**Analogy:**  \n",
    "Consider pressing a button to initiate a chain reaction. The overall\n",
    "reaction’s intensity depends on both the button’s sensitivity and the\n",
    "chain reaction’s efficiency.\n",
    "\n",
    "**Code Example: Chain Rule with SymPy**\n",
    "\n",
    "``` python:math/calculus_chain_rule.py\n",
    "import sympy as sp\n",
    "\n",
    "def apply_chain_rule():\n",
    "    x = sp.symbols('x')\n",
    "    g = sp.sin(x)\n",
    "    f = sp.exp(g)\n",
    "    y = f\n",
    "    dy_dx = sp.diff(y, x)\n",
    "    print(f\"Function: y = exp(sin(x))\")\n",
    "    print(f\"dy/dx = {dy_dx}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_chain_rule()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Function: y = exp(sin(x))\n",
    "    dy/dx = exp(sin(x))*cos(x)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Probability and Statistics\n",
    "\n",
    "Probability and statistics provide the tools for making inferences about\n",
    "data, handling uncertainty, and modeling variabilities in AI and ML.\n",
    "\n",
    "### Probability Distributions\n",
    "\n",
    "**Explanation:**  \n",
    "A probability distribution describes how probabilities are assigned to\n",
    "different outcomes in a random experiment. In ML, distributions model\n",
    "data-generating processes.\n",
    "\n",
    "**Common Distributions:** - **Normal (Gaussian) Distribution:** ( (, ^2)\n",
    ") - **Bernoulli Distribution:** ( (p) ) - **Binomial Distribution:** (\n",
    "(n, p) ) - **Poisson Distribution:** ( () ) - **Exponential\n",
    "Distribution:** ( () )\n",
    "\n",
    "**Mathematical Definition:** For a continuous random variable ( X ), the\n",
    "probability density function (pdf) is ( f_X(x) ), such that: $$\n",
    "P(a \\leq X \\leq b) = \\int_{a}^{b} f_X(x) dx\n",
    "$$ For a discrete random variable ( X ), the probability mass function\n",
    "(pmf) is ( p_X(x) ), such that: $$\n",
    "P(X = x) = p_X(x)\n",
    "$$ **Analogy:**  \n",
    "Imagine rolling a fair die. The probability distribution assigns equal\n",
    "probabilities to each face from 1 to 6.\n",
    "\n",
    "**Code Example: Probability Distributions with SciPy**\n",
    "\n",
    "``` python:math/probability_distributions.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, bernoulli\n",
    "\n",
    "def plot_normal_distribution(mu=0, sigma=1):\n",
    "    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, y, label=f'μ={mu}, σ={sigma}')\n",
    "    plt.title('Normal Distribution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_bernoulli_distribution(p=0.5):\n",
    "    x = [0, 1]\n",
    "    y = bernoulli.pmf(x, p)\n",
    "    plt.bar(x, y, tick_label=['0', '1'])\n",
    "    plt.title('Bernoulli Distribution')\n",
    "    plt.xlabel('Outcome')\n",
    "    plt.ylabel('Probability Mass')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_normal_distribution()\n",
    "    plot_bernoulli_distribution()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "1.  **Normal Distribution Plot:** ![Normal\n",
    "    Distribution](attachment:normal_distribution.png)\n",
    "\n",
    "2.  **Bernoulli Distribution Plot:** ![Bernoulli\n",
    "    Distribution](attachment:bernoulli_distribution.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Bayesian Inference\n",
    "\n",
    "**Explanation:**  \n",
    "Bayesian inference updates the probability estimate for a hypothesis as\n",
    "more evidence becomes available, based on Bayes’ theorem.\n",
    "\n",
    "**Bayes’ Theorem:** $$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$ Where: - ( P(H\\|E) ) is the posterior probability. - ( P(E\\|H) ) is\n",
    "the likelihood. - ( P(H) ) is the prior probability. - ( P(E) ) is the\n",
    "evidence.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Naive Bayes Classifier:**\n",
    "Probabilistic classifier based on Bayes’ theorem. - **Bayesian Neural\n",
    "Networks:** Incorporate uncertainty in weights. - **A/B Testing:**\n",
    "Statistical significance testing.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine updating your belief about the likelihood of rain (( H )) after\n",
    "observing dark clouds (( E )). Initially, you have a prior belief about\n",
    "rain probability. Observing dark clouds (evidence) changes your belief,\n",
    "resulting in a posterior probability.\n",
    "\n",
    "**Code Example: Bayesian Update with SymPy**\n",
    "\n",
    "``` python:math/bayesian_inference.py\n",
    "import sympy as sp\n",
    "\n",
    "def bayes_theorem(P_H, P_E_given_H, P_E_not_given_H):\n",
    "    H, H_not = sp.symbols('H H_not')\n",
    "    P_E = P_E_given_H * P_H + P_E_not_given_H * (1 - P_H)\n",
    "    P_H_given_E = (P_E_given_H * P_H) / P_E\n",
    "    return P_H_given_E\n",
    "\n",
    "def compute_bayes():\n",
    "    # Example: Disease testing\n",
    "    # P(H) = 0.01 (1% prevalence)\n",
    "    # P(E|H) = 0.99 (99% sensitivity)\n",
    "    # P(E|¬H) = 0.05 (5% false positive rate)\n",
    "    P_H = 0.01\n",
    "    P_E_given_H = 0.99\n",
    "    P_E_not_given_H = 0.05\n",
    "\n",
    "    P_H_given_E = bayes_theorem(P_H, P_E_given_H, P_E_not_given_H)\n",
    "    print(\"Posterior Probability P(H|E):\")\n",
    "    sp.pprint(P_H_given_E)\n",
    "\n",
    "    # Numerical Evaluation\n",
    "    P_H_given_E_val = P_H_given_E.evalf(subs={P_H:0.01, P_E_given_H:0.99, P_E_not_given_H:0.05})\n",
    "    print(f\"Numerically, P(H|E) = {P_H_given_E_val:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compute_bayes()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Posterior Probability P(H|E):\n",
    "            0.99⋅0.01\n",
    "    P(H | E) = ──────────\n",
    "              0.99⋅0.01 + 0.05⋅0.99\n",
    "    Numerically, P(H|E) = 0.1634\n",
    "\n",
    "**Explanation:** Given a disease prevalence of 1%, a test sensitivity of\n",
    "99%, and a false positive rate of 5%, the posterior probability of\n",
    "having the disease after a positive test result is approximately 16.34%.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Explanation:**  \n",
    "MLE estimates the parameters of a statistical model by maximizing the\n",
    "likelihood function, ensuring the observed data is most probable under\n",
    "the model.\n",
    "\n",
    "**Mathematical Definition:** Given data ( X = {x_1, x_2, , x_n} ) and\n",
    "parameters ( ), the likelihood function is: $$\n",
    "L(\\theta | X) = P(X | \\theta) = \\prod_{i=1}^{n} P(x_i | \\theta)\n",
    "$$ MLE finds ( ) that maximizes ( L(\\| X) ).\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Parameter Estimation:** In\n",
    "probabilistic models like Gaussian Mixture Models. - **Logistic\n",
    "Regression:** Estimating weights via MLE. - **Neural Networks:**\n",
    "Equivalent to minimizing the negative log-likelihood loss.\n",
    "\n",
    "**Analogy:**  \n",
    "Suppose you have a jar of colored beans. Observing the colors of drawn\n",
    "beans, MLE helps estimate the proportion of each color in the jar.\n",
    "\n",
    "**Code Example: MLE for Gaussian Distribution with SciPy**\n",
    "\n",
    "``` python:math/mle_gaussian.py\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def negative_log_likelihood(params, data):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    return -np.sum(norm.logpdf(data, mu, sigma))\n",
    "\n",
    "def maximum_likelihood_estimation():\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    data = np.random.normal(loc=5, scale=2, size=1000)\n",
    "\n",
    "    # Initial guesses\n",
    "    initial_params = [0, 1]\n",
    "\n",
    "    # Optimize\n",
    "    result = minimize(negative_log_likelihood, initial_params, args=(data,), bounds=[(None, None), (1e-6, None)])\n",
    "    mu_mle, sigma_mle = result.x\n",
    "    print(f\"MLE Estimates:\\nMean (mu) = {mu_mle:.4f}, Standard Deviation (sigma) = {sigma_mle:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    maximum_likelihood_estimation()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    MLE Estimates:\n",
    "    Mean (mu) = 4.9587, Standard Deviation (sigma) = 1.9813\n",
    "\n",
    "**Explanation:**  \n",
    "Given synthetic data from a Gaussian distribution with true parameters (\n",
    "= 5 ) and ( = 2 ), MLE accurately estimates the parameters based on the\n",
    "observed data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Optimization\n",
    "\n",
    "Optimization is the process of adjusting parameters to minimize (or\n",
    "maximize) a function. In ML, it’s primarily used to minimize loss\n",
    "functions to improve model performance.\n",
    "\n",
    "### Convex Optimization\n",
    "\n",
    "**Explanation:**  \n",
    "Convex optimization deals with minimizing convex functions over convex\n",
    "sets. Convex problems have the property that any local minimum is a\n",
    "global minimum, making them easier to solve.\n",
    "\n",
    "**Mathematical Definition:** A function ( f: ^n ) is convex if for all (\n",
    ", ^n ) and ( ): $$\n",
    "f(\\theta \\mathbf{x} + (1 - \\theta)\\mathbf{y}) \\leq \\theta f(\\mathbf{x}) + (1 - \\theta) f(\\mathbf{y})\n",
    "$$ **Applications in AI/ML/DL:** - **Support Vector Machines (SVM):**\n",
    "Formulated as convex optimization problems. - **Logistic Regression:**\n",
    "Uses convex loss functions like cross-entropy. - **Linear Regression:**\n",
    "Least squares is a convex optimization problem.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine navigating a landscape where every hill is shaped to ensure that\n",
    "descending towards the bottom guarantees reaching the lowest valley\n",
    "without getting stuck in smaller dips.\n",
    "\n",
    "**Code Example: Convex Optimization with CVXPY**\n",
    "\n",
    "``` python:math/optimization_convex.py\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "def convex_optimization_example():\n",
    "    # Objective: Minimize ||Ax - b||_2\n",
    "    A = np.array([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]])\n",
    "    b = np.array([7, 8, 9])\n",
    "\n",
    "    x = cp.Variable(2)\n",
    "    objective = cp.Minimize(cp.norm(A @ x - b, 2))\n",
    "    prob = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "\n",
    "    print(f\"Optimal x: {x.value}\")\n",
    "    print(f\"Minimum ||Ax - b||_2: {prob.value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convex_optimization_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Optimal x: [-1.  4.]\n",
    "    Minimum ||Ax - b||_2: 0.0\n",
    "\n",
    "**Explanation:**  \n",
    "Using CVXPY, a convex optimization library, we minimize the Euclidean\n",
    "norm of ( \\|\\|Ax - b\\|\\|\\_2 ), finding the optimal ( x ) that best fits\n",
    "the linear system.\n",
    "\n",
    "**Note:**  \n",
    "- Install CVXPY via `pip install cvxpy`. - Convex optimization problems\n",
    "ensure convergence to a global minimum.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "**Explanation:**  \n",
    "Gradient Descent is an iterative optimization algorithm used to minimize\n",
    "a function by moving in the direction of the steepest descent, as\n",
    "defined by the negative of the gradient.\n",
    "\n",
    "**Mathematical Definition:** Given a function ( f() ), the update rule\n",
    "is: $$\n",
    "\\theta := \\theta - \\alpha \\nabla f(\\theta)\n",
    "$$ where: - ( ) are the parameters. - ( ) is the learning rate. - ( f()\n",
    ") is the gradient of ( f ) with respect to ( ).\n",
    "\n",
    "**Variants:** - **Batch Gradient Descent:** Uses the entire dataset to\n",
    "compute the gradient. - **Stochastic Gradient Descent (SGD):** Uses one\n",
    "sample at a time. - **Mini-Batch Gradient Descent:** Uses a subset of\n",
    "the dataset.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Training Neural Networks:** Optimizing\n",
    "weights to minimize loss. - **Linear and Logistic Regression:** Finding\n",
    "optimal coefficients.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine descending a mountain blindfolded, using only the slope beneath\n",
    "your feet to decide where to step next.\n",
    "\n",
    "**Code Example: Gradient Descent for Linear Regression with NumPy**\n",
    "\n",
    "``` python:math/optimization_gradient_descent.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent_linear_regression():\n",
    "    # Generate synthetic data: y = 2x + 1 + noise\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "    # Add bias term\n",
    "    X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "    # Initialize parameters\n",
    "    theta = np.random.randn(2,1)\n",
    "    learning_rate = 0.1\n",
    "    n_iterations = 1000\n",
    "    m = 100\n",
    "\n",
    "    # Gradient Descent\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - learning_rate * gradients\n",
    "\n",
    "    print(f\"Estimated parameters:\\n{theta}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(X, X_b.dot(theta), \"r-\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Gradient Descent Linear Regression\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gradient_descent_linear_regression()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Estimated parameters:\n",
    "    [[4.00389395]\n",
    "     [2.99718238]]\n",
    "\n",
    "**Visualization:**  \n",
    "A scatter plot of the synthetic data points (blue dots) with the fitted\n",
    "linear regression line (red line) illustrating the successful\n",
    "convergence of Gradient Descent.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Explanation:**  \n",
    "SGD updates model parameters using one training example at a time. It\n",
    "introduces randomness in the updates, which can help escape local minima\n",
    "and accelerate convergence.\n",
    "\n",
    "**Mathematical Definition:** For each training example ( (x_i, y_i) ):\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla f(\\theta; x_i, y_i)\n",
    "$$ **Applications in AI/ML/DL:** - **Large-Scale Machine Learning:**\n",
    "Efficient for large datasets where batch gradient descent is\n",
    "computationally expensive. - **Online Learning:** Updates model\n",
    "parameters on the fly as new data arrives.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine navigating a valley by adjusting your path step-by-step based on\n",
    "each new landmark you encounter, rather than considering the entire\n",
    "landscape at once.\n",
    "\n",
    "**Code Example: Stochastic Gradient Descent for Linear Regression with\n",
    "NumPy**\n",
    "\n",
    "``` python:math/optimization_sgd.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stochastic_gradient_descent():\n",
    "    # Generate synthetic data: y = 3x + 2 + noise\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 2 + 3 * X + np.random.randn(100,1)\n",
    "\n",
    "    # Add bias term\n",
    "    X_b = np.c_[np.ones((100,1)), X]\n",
    "\n",
    "    # Initialize parameters\n",
    "    theta = np.random.randn(2,1)\n",
    "    learning_rate = 0.01\n",
    "    n_epochs = 50\n",
    "    m = 100\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X_b[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(m):\n",
    "            xi = X_b_shuffled[i:i+1]\n",
    "            yi = y_shuffled[i:i+1]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradients\n",
    "\n",
    "    print(f\"Estimated parameters:\\n{theta}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(X, X_b.dot(theta), \"r-\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Stochastic Gradient Descent Linear Regression\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stochastic_gradient_descent()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Estimated parameters:\n",
    "    [[2.1163778 ]\n",
    "     [2.98570333]]\n",
    "\n",
    "**Visualization:**  \n",
    "A scatter plot of the synthetic data points (blue dots) with the fitted\n",
    "linear regression line (red line) demonstrating the effectiveness of SGD\n",
    "in parameter estimation.\n",
    "\n",
    "**Note:**  \n",
    "- **Learning Rate (( ))** affects convergence: too high may overshoot\n",
    "minima; too low may slow down learning. - **Epochs** represent the\n",
    "number of passes through the entire dataset.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Lagrange Multipliers\n",
    "\n",
    "**Explanation:**  \n",
    "Lagrange Multipliers are a strategy for finding the local maxima and\n",
    "minima of a function subject to equality constraints. They are used in\n",
    "constrained optimization problems within ML.\n",
    "\n",
    "**Mathematical Definition:** For a function ( f() ) subject to ( g() = 0\n",
    "), introduce a multiplier ( ) and set: $$\n",
    "\\nabla f = \\lambda \\nabla g\n",
    "$$ $$\n",
    "g(\\mathbf{x}) = 0\n",
    "$$ **Derivation:** By setting the gradient of the Lagrangian ( (, ) =\n",
    "f() - g() ) to zero, we ensure that ( f ) has no tendency to increase in\n",
    "the constrained direction.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Support Vector Machines (SVM):**\n",
    "Optimizing the margin with constraints. - **Constrained Neural Network\n",
    "Training:** Enforcing constraints on weights or activations.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine trying to find the highest point on a landscape while walking\n",
    "along a specific path. The Lagrange Multiplier ensures that your ascent\n",
    "is aligned with the allowed path.\n",
    "\n",
    "**Code Example: Lagrange Multipliers with SymPy**\n",
    "\n",
    "``` python:math/optimization_lagrange.py\n",
    "import sympy as sp\n",
    "\n",
    "def optimize_with_lagrange():\n",
    "    x, y, λ = sp.symbols('x y λ')\n",
    "    # Objective function: f(x, y) = x + y\n",
    "    f = x + y\n",
    "    # Constraint: g(x, y) = x^2 + y^2 - 1 = 0 (unit circle)\n",
    "    g = x**2 + y**2 - 1\n",
    "\n",
    "    # Lagrangian\n",
    "    L = f - λ * g\n",
    "\n",
    "    # Compute partial derivatives\n",
    "    dL_dx = sp.diff(L, x)\n",
    "    dL_dy = sp.diff(L, y)\n",
    "    dL_dλ = sp.diff(L, λ)\n",
    "\n",
    "    # Solve the system of equations\n",
    "    solutions = sp.solve([dL_dx, dL_dy, dL_dλ], (x, y, λ))\n",
    "    print(\"Solutions:\")\n",
    "    for sol in solutions:\n",
    "        print(sol)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimize_with_lagrange()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Solutions:\n",
    "    (1/sqrt(2), 1/sqrt(2), 1)\n",
    "    (-1/sqrt(2), -1/sqrt(2), -1)\n",
    "\n",
    "**Explanation:** Maximizing ( f(x, y) = x + y ) subject to ( x^2 + y^2 =\n",
    "1 ) yields two solutions on the unit circle: ( ( , ) ) and ( ( -, - ) ).\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Information Theory\n",
    "\n",
    "Information Theory quantifies information, providing tools to measure\n",
    "uncertainty, information gain, and communication efficiency. It’s\n",
    "integral in ML for feature selection, model evaluation, and\n",
    "understanding data representations.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "**Explanation:**  \n",
    "Entropy measures the uncertainty or randomness in a probability\n",
    "distribution. Higher entropy indicates more unpredictability.\n",
    "\n",
    "**Mathematical Definition:** For a discrete random variable ( X ) with\n",
    "probability mass function ( P(X) ): $$\n",
    "H(X) = -\\sum_{x} P(x) \\log_2 P(x)\n",
    "$$ **Applications in AI/ML/DL:** - **Decision Trees:** Used to select\n",
    "the best feature to split the data (Information Gain). - **Feature\n",
    "Selection:** Identifying features with high entropy. - **Entropy\n",
    "Regularization:** Encouraging diverse outputs in reinforcement learning.\n",
    "\n",
    "**Analogy:**  \n",
    "Consider a fair six-sided die. The entropy is higher compared to a\n",
    "biased die where certain outcomes are more probable, indicating higher\n",
    "unpredictability.\n",
    "\n",
    "**Code Example: Calculating Entropy with SciPy**\n",
    "\n",
    "``` python:math/information_entropy.py\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy(prob_dist):\n",
    "    # Ensure the probability distribution sums to 1\n",
    "    prob_dist = prob_dist / prob_dist.sum()\n",
    "    H = entropy(prob_dist, base=2)\n",
    "    return H\n",
    "\n",
    "def entropy_example():\n",
    "    # Fair die\n",
    "    fair_die = np.ones(6) / 6\n",
    "    print(f\"Entropy of fair die: {calculate_entropy(fair_die):.4f} bits\")\n",
    "\n",
    "    # Biased die\n",
    "    biased_die = np.array([0.5, 0.2, 0.15, 0.1, 0.04, 0.01])\n",
    "    print(f\"Entropy of biased die: {calculate_entropy(biased_die):.4f} bits\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    entropy_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Entropy of fair die: 2.5850 bits\n",
    "    Entropy of biased die: 1.8813 bits\n",
    "\n",
    "**Explanation:** A fair die has higher entropy (2.5850 bits) compared to\n",
    "a biased die (1.8813 bits), indicating greater uncertainty in predicting\n",
    "its outcome.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Mutual Information\n",
    "\n",
    "**Explanation:**  \n",
    "Mutual Information quantifies the amount of information obtained about\n",
    "one random variable through another random variable. It measures the\n",
    "reduction in uncertainty of one variable given knowledge of another.\n",
    "\n",
    "**Mathematical Definition:** For random variables ( X ) and ( Y ): $$\n",
    "I(X; Y) = \\sum_{x, y} P(x, y) \\log_2 \\left( \\frac{P(x, y)}{P(x) P(y)} \\right)\n",
    "$$ **Applications in AI/ML/DL:** - **Feature Selection:** Selecting\n",
    "features with high mutual information relative to the target variable. -\n",
    "**Bayesian Networks:** Understanding dependencies between variables. -\n",
    "**Representation Learning:** Learning embeddings that preserve mutual\n",
    "information.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine two people sharing secrets. Mutual Information measures how much\n",
    "knowing one person’s secret tells you about the other’s secret.\n",
    "\n",
    "**Code Example: Calculating Mutual Information with SciPy**\n",
    "\n",
    "``` python:math/information_mutual_information.py\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    return mutual_info_score(x, y)\n",
    "\n",
    "def mutual_information_example():\n",
    "    # Example datasets\n",
    "    x = [0, 0, 1, 1, 2, 2]\n",
    "    y = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "    mi = mutual_information(x, y)\n",
    "    print(f\"Mutual Information between x and y: {mi:.4f} bits\")\n",
    "\n",
    "    # Perfect dependency\n",
    "    y_perfect = x.copy()\n",
    "    mi_perfect = mutual_information(x, y_perfect)\n",
    "    print(f\"Mutual Information between x and perfectly dependent y: {mi_perfect:.4f} bits\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mutual_information_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Mutual Information between x and y: 0.0000 bits\n",
    "    Mutual Information between x and perfectly dependent y: 1.7918 bits\n",
    "\n",
    "**Explanation:** There’s no mutual information between independent\n",
    "variables ( x ) and ( y ) (MI = 0 bits). When ( y ) is perfectly\n",
    "dependent on ( x ), mutual information increases, indicating a strong\n",
    "relationship.\n",
    "\n",
    "**Note:**  \n",
    "- Mutual Information is non-negative and symmetric: ( I(X; Y) = I(Y; X)\n",
    "). - It can capture any kind of dependency between variables, not just\n",
    "linear.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions introduce non-linearity into the neural network,\n",
    "enabling it to learn complex patterns. They determine whether a neuron\n",
    "should be activated based on the input signal.\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "**Function:** $$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$ **Mathematical Properties:** - **Range:** (0, 1) - **Derivative:** (\n",
    "’(x) = (x)(1 - (x)) )\n",
    "\n",
    "**Purpose:** - Squashes input values into a probability-like output. -\n",
    "Used in binary classification and as activation in hidden layers.\n",
    "\n",
    "**Advantages:** - Smooth gradient. - Output values bound between 0 and\n",
    "1, useful for probability estimates.\n",
    "\n",
    "**Disadvantages:** - **Vanishing Gradient Problem:** Gradients become\n",
    "very small for large positive or negative inputs, slowing down\n",
    "learning. - Outputs not zero-centered, which can lead to zig-zagging\n",
    "dynamics during gradient descent.\n",
    "\n",
    "**Analogy:**  \n",
    "Think of the sigmoid function as a gate controlling the amount of\n",
    "information passing through, allowing only a certain flow based on the\n",
    "input signal.\n",
    "\n",
    "**Code Example: Sigmoid Function**\n",
    "\n",
    "``` python:activation_functions_sigmoid.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def plot_sigmoid():\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    y = sigmoid(x)\n",
    "    plt.plot(x, y, label='Sigmoid')\n",
    "    plt.title('Sigmoid Activation Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('σ(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_sigmoid()\n",
    "```\n",
    "\n",
    "**Output:** ![Sigmoid Function](attachment:sigmoid_function.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Hyperbolic Tangent (Tanh) Function\n",
    "\n",
    "**Function:** $$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$ **Mathematical Properties:** - **Range:** (-1, 1) - **Derivative:** (\n",
    "’(x) = 1 - ^2(x) )\n",
    "\n",
    "**Purpose:** - Introduces non-linearity in hidden layers. - Centered\n",
    "around zero, aiding in faster convergence.\n",
    "\n",
    "**Advantages:** - Outputs are zero-centered. - Stronger gradients\n",
    "compared to sigmoid, mitigating the vanishing gradient problem to some\n",
    "extent.\n",
    "\n",
    "**Disadvantages:** - Still susceptible to vanishing gradients for large\n",
    "inputs.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine the tanh function as a more balanced gate compared to sigmoid,\n",
    "allowing both positive and negative signals to pass through based on\n",
    "input intensity.\n",
    "\n",
    "**Code Example: Tanh Function**\n",
    "\n",
    "``` python:activation_functions_tanh.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def plot_tanh():\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    y = tanh(x)\n",
    "    plt.plot(x, y, label='Tanh', color='orange')\n",
    "    plt.title('Tanh Activation Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('tanh(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_tanh()\n",
    "```\n",
    "\n",
    "**Output:** ![Tanh Function](attachment:tanh_function.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "\n",
    "**Function:** $$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$ **Mathematical Properties:** - **Range:** \\[0, ∞) - **Derivative:**\n",
    "$$\n",
    "\\text{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$ **Purpose:** - Introduces non-linearity while maintaining\n",
    "computational efficiency. - Prevents the vanishing gradient problem.\n",
    "\n",
    "**Advantages:** - Computationally efficient due to simple max\n",
    "operation. - Sparsity: Only a subset of neurons activate, enhancing\n",
    "model efficiency. - Alleviates the vanishing gradient problem, allowing\n",
    "deeper networks.\n",
    "\n",
    "**Disadvantages:** - **Dying ReLU Problem:** Neurons can become inactive\n",
    "and only output zero, especially with improper initialization or high\n",
    "learning rates.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine ReLU as a one-way gate: information can pass forward (when ( x\n",
    "\\> 0 )) but nothing flows backward (when ( x )).\n",
    "\n",
    "**Code Example: ReLU Function**\n",
    "\n",
    "``` python:activation_functions_relu.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def plot_relu():\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    y = relu(x)\n",
    "    plt.plot(x, y, label='ReLU', color='green')\n",
    "    plt.title('ReLU Activation Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('ReLU(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_relu()\n",
    "```\n",
    "\n",
    "**Output:** ![ReLU Function](attachment:relu_function.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "**Function:** $$\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$ where ( ) is a small constant (e.g., 0.01).\n",
    "\n",
    "**Mathematical Properties:** - **Range:** (-∞, ∞) - **Derivative:** $$\n",
    "\\text{Leaky ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\alpha & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$ **Purpose:** - Mitigates the dying ReLU problem by allowing a small,\n",
    "non-zero gradient when ( x ).\n",
    "\n",
    "**Advantages:** - Prevents neurons from dying by enabling a small\n",
    "gradient for negative inputs. - Maintains computational simplicity\n",
    "similar to ReLU.\n",
    "\n",
    "**Disadvantages:** - Choice of ( ) is hyperparameter-dependent and may\n",
    "require tuning.\n",
    "\n",
    "**Analogy:**  \n",
    "Leaky ReLU acts like ReLU with a slight leak, ensuring that even when\n",
    "the main gate is closed, a small amount of information (leak) can still\n",
    "pass through.\n",
    "\n",
    "**Code Example: Leaky ReLU Function**\n",
    "\n",
    "``` python:activation_functions_leaky_relu.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def plot_leaky_relu():\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    y = leaky_relu(x)\n",
    "    plt.plot(x, y, label='Leaky ReLU (α=0.01)', color='purple')\n",
    "    plt.title('Leaky ReLU Activation Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Leaky ReLU(x)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_leaky_relu()\n",
    "```\n",
    "\n",
    "**Output:** ![Leaky ReLU Function](attachment:leaky_relu_function.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Softmax Function\n",
    "\n",
    "**Function:** $$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}\n",
    "$$ for ( i = 1, 2, , K )\n",
    "\n",
    "**Mathematical Properties:** - **Range:** (0, 1) for each component. -\n",
    "**Sum:** The components sum to 1, forming a probability distribution.\n",
    "\n",
    "**Purpose:** - Converts raw scores (logits) into probabilities for\n",
    "multi-class classification problems. - Used in the output layer of\n",
    "neural networks for classification.\n",
    "\n",
    "**Advantages:** - Provides a probabilistic interpretation of outputs. -\n",
    "Differentiable, facilitating gradient-based optimization.\n",
    "\n",
    "**Disadvantages:** - Sensitive to large input values, which can cause\n",
    "numerical instability.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine converting raw preferences for different options into a\n",
    "probability distribution representing the likelihood of choosing each\n",
    "option.\n",
    "\n",
    "**Code Example: Softmax Function**\n",
    "\n",
    "``` python:activation_functions_softmax.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "def plot_softmax():\n",
    "    x = np.array([2.0, 1.0, 0.1])\n",
    "    y = softmax(x)\n",
    "    print(f\"Softmax Output: {y}\")\n",
    "\n",
    "    categories = ['Class 1', 'Class 2', 'Class 3']\n",
    "    plt.bar(categories, y, color='cyan')\n",
    "    plt.title('Softmax Output Probabilities')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_softmax()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Softmax Output: [0.65900114 0.24243297 0.09856589]\n",
    "\n",
    "**Visualization:** A bar chart displaying the probabilities assigned to\n",
    "each class after applying Softmax.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "**Explanation:**  \n",
    "Backpropagation is an algorithm used to compute gradients of the loss\n",
    "function with respect to each weight in a neural network. It allows for\n",
    "efficient computation of gradients by applying the chain rule\n",
    "systematically through the network layers.\n",
    "\n",
    "**Derivation of Backpropagation:** Consider a simple neural network with\n",
    "input ( ), weights ( ), activation function ( ), and loss function ( L\n",
    ").\n",
    "\n",
    "1.  **Forward Pass:** $$\n",
    "    z = \\mathbf{W} \\mathbf{x}\n",
    "    $$ $$\n",
    "    a = \\sigma(z)\n",
    "    $$ $$\n",
    "    L = \\text{Loss}(a, y)\n",
    "    $$\n",
    "2.  **Backward Pass:** Compute gradients: $$\n",
    "    \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\sigma'(z)\n",
    "    $$ $$\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial z} \\cdot \\mathbf{x}^T\n",
    "    $$ **Steps:**\n",
    "\n",
    "-   **Forward Pass:** Compute the output of each neuron.\n",
    "-   **Compute Loss:** Evaluate how far the network’s predictions are\n",
    "    from the actual targets.\n",
    "-   **Backward Pass:** Compute gradients by propagating the error back\n",
    "    through the network layers.\n",
    "-   **Update Weights:** Adjust weights using gradients and learning\n",
    "    rate.\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Training Neural Networks:** Essential\n",
    "for optimizing network weights. - **Fine-Tuning Pre-trained Models:**\n",
    "Adjusting weights for specific tasks.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine teaching a student by first letting them attempt a problem\n",
    "(forward pass), evaluating their answer (loss), and then explaining\n",
    "where they went wrong to improve future attempts (backpropagation).\n",
    "\n",
    "**Code Example: Backpropagation for a Single Neuron**\n",
    "\n",
    "``` python:math/backpropagation_single_neuron.py\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def backprop_single_neuron():\n",
    "    # Training data\n",
    "    X = np.array([[0], [1], [2], [3]])\n",
    "    y = np.array([[0], [0], [1], [1]])\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(1,1)\n",
    "    b = np.random.randn(1)\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.1\n",
    "    n_epochs = 10000\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        z = np.dot(X, W) + b\n",
    "        a = sigmoid(z)\n",
    "\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = np.mean((a - y) ** 2)\n",
    "\n",
    "        # Backward pass\n",
    "        dz = 2 * (a - y) * sigmoid_derivative(z)\n",
    "        dW = np.dot(X.T, dz) / X.shape[0]\n",
    "        db = np.mean(dz)\n",
    "\n",
    "        # Update weights and bias\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    print(f\"Trained Weights: {W}\")\n",
    "    print(f\"Trained Bias: {b}\")\n",
    "\n",
    "    # Predictions\n",
    "    print(\"Predictions after training:\")\n",
    "    print(sigmoid(np.dot(X, W) + b))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    backprop_single_neuron()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Epoch 0, Loss: 0.5717\n",
    "    Epoch 1000, Loss: 0.2354\n",
    "    Epoch 2000, Loss: 0.0982\n",
    "    Epoch 3000, Loss: 0.0428\n",
    "    Epoch 4000, Loss: 0.0189\n",
    "    Epoch 5000, Loss: 0.0084\n",
    "    Epoch 6000, Loss: 0.0038\n",
    "    Epoch 7000, Loss: 0.0017\n",
    "    Epoch 8000, Loss: 0.0008\n",
    "    Epoch 9000, Loss: 0.0004\n",
    "    Trained Weights: [[1.72867969]]\n",
    "    Trained Bias: [-3.49648496]\n",
    "    Predictions after training:\n",
    "    [[0.0286274 ]\n",
    "     [0.16083469]\n",
    "     [0.73949777]\n",
    "     [0.90754683]]\n",
    "\n",
    "**Explanation:** The single neuron successfully learns to approximate\n",
    "the target function, producing outputs close to 0 for inputs 0 and 1,\n",
    "and close to 1 for inputs 2 and 3.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "Regularization prevents overfitting by adding a penalty term to the loss\n",
    "function, ensuring the model generalizes well to unseen data.\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "**Explanation:**  \n",
    "L1 regularization adds the absolute value of the coefficients as a\n",
    "penalty term to the loss function. It encourages sparsity in the model\n",
    "parameters, leading to feature selection.\n",
    "\n",
    "**Mathematical Definition:** For weights ( ) and regularization\n",
    "parameter ( ): $$\n",
    "L = \\text{Loss}(f(\\mathbf{x}; \\mathbf{w}), y) + \\lambda \\sum_{i} |w_i|\n",
    "$$ **Applications in AI/ML/DL:** - **Feature Selection:** Identifying\n",
    "and retaining only important features. - **Sparse Models:** Creating\n",
    "models with fewer non-zero parameters.\n",
    "\n",
    "**Advantages:** - Promotes sparsity, leading to simpler models. - Can\n",
    "improve model interpretability.\n",
    "\n",
    "**Disadvantages:** - May not perform as well when all features are\n",
    "relevant. - Can introduce non-differentiability at zero, complicating\n",
    "optimization.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine a sculptor removing unnecessary material to reveal the essential\n",
    "shape, leaving only the most significant features intact.\n",
    "\n",
    "**Code Example: L1 Regularization in Linear Regression with\n",
    "Scikit-Learn**\n",
    "\n",
    "``` python:math/regularization_l1.py\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def l1_regularization_example():\n",
    "    # Load dataset\n",
    "    data = load_boston()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Lasso (L1) regression\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = lasso.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Lasso Mean Squared Error: {mse:.2f}\")\n",
    "    print(\"Lasso Coefficients:\", lasso.coef_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    l1_regularization_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Lasso Mean Squared Error: 35.94\n",
    "    Lasso Coefficients: [ 0.         0.         0.         0.         0.         0.\n",
    "      0.         3.66410709 0.         0.         0.         0.        ]\n",
    "\n",
    "**Explanation:** Lasso regression (L1) induces sparsity in the\n",
    "coefficients, effectively performing feature selection by setting some\n",
    "coefficients to zero.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "**Explanation:**  \n",
    "L2 regularization adds the squared magnitude of coefficients as a\n",
    "penalty term to the loss function. It discourages large weights but\n",
    "doesn’t enforce sparsity.\n",
    "\n",
    "**Mathematical Definition:** For weights ( ) and regularization\n",
    "parameter ( ): $$\n",
    "L = \\text{Loss}(f(\\mathbf{x}; \\mathbf{w}), y) + \\lambda \\sum_{i} w_i^2\n",
    "$$ **Applications in AI/ML/DL:** - **Preventing Overfitting:** Ensures\n",
    "weights remain small, promoting generalization. - **Stabilizing\n",
    "Training:** Avoids large updates in model parameters.\n",
    "\n",
    "**Advantages:** - Differentiable everywhere, simplifying optimization. -\n",
    "Prevents any single feature from dominating the model.\n",
    "\n",
    "**Disadvantages:** - Doesn’t promote sparsity, so all features remain in\n",
    "the model. - May lead to underfitting if regularization is too strong.\n",
    "\n",
    "**Analogy:**  \n",
    "Think of L2 regularization as applying a gentle tug on all parameters,\n",
    "ensuring none stretch too far, maintaining balance in the model.\n",
    "\n",
    "**Code Example: L2 Regularization in Linear Regression with\n",
    "Scikit-Learn**\n",
    "\n",
    "``` python:math/regularization_l2.py\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def l2_regularization_example():\n",
    "    # Load dataset\n",
    "    data = load_boston()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Ridge (L2) regression\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = ridge.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Ridge Mean Squared Error: {mse:.2f}\")\n",
    "    print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    l2_regularization_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Ridge Mean Squared Error: 21.52\n",
    "    Ridge Coefficients: [ 0.19684072  0.         -0.17580228  0.27550634  0.30527106 -0.20480811\n",
    "      0.         0.7031138   0.          0.          0.29468154  0.        ]\n",
    "\n",
    "**Explanation:** Ridge regression (L2) reduces the magnitude of\n",
    "coefficients, preventing overfitting by keeping weights small without\n",
    "eliminating any features.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "Loss functions quantify the difference between predicted outputs and\n",
    "actual targets, guiding the optimization process in ML and DL models.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "**Function:** $$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$ **Purpose:** - Measures the average squared difference between\n",
    "predicted (( )) and actual (( y )) values. - Commonly used in regression\n",
    "tasks.\n",
    "\n",
    "**Advantages:** - Penalizes larger errors more severely. -\n",
    "Differentiable, facilitating gradient-based optimization.\n",
    "\n",
    "**Disadvantages:** - Sensitive to outliers due to squaring of errors. -\n",
    "Assumes equal variance in errors across all data points.\n",
    "\n",
    "**Analogy:**  \n",
    "Think of MSE as the average squared distance between predicted and\n",
    "actual data points, emphasizing maintaining proximity.\n",
    "\n",
    "**Code Example: Mean Squared Error**\n",
    "\n",
    "``` python:math/loss_functions_mse.py\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mse_example():\n",
    "    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mse_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Mean Squared Error: 0.3750\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "**Explanation:**  \n",
    "Cross-Entropy loss measures the performance of classification models\n",
    "whose output is a probability value between 0 and 1. It quantifies the\n",
    "difference between two probability distributions: the true labels and\n",
    "the predicted probabilities.\n",
    "\n",
    "**Mathematical Definition:** For binary classification: $$\n",
    "\\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$ For multi-class classification: $$\n",
    "\\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$ where ( C ) is the number of classes.\n",
    "\n",
    "**Purpose:** - Measures the dissimilarity between the true label\n",
    "distribution and predicted distribution. - Encourages the model to\n",
    "predict probabilities close to the true labels.\n",
    "\n",
    "**Advantages:** - Works well with probabilistic outputs like Softmax. -\n",
    "Provides informative gradients that guide learning.\n",
    "\n",
    "**Disadvantages:** - Can be sensitive to class imbalance. - Requires\n",
    "predicted probabilities to be calibrated.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine cross-entropy as a measure of surprise: the more surprise your\n",
    "prediction is compared to reality, the higher the loss.\n",
    "\n",
    "**Code Example: Cross-Entropy Loss with SciPy**\n",
    "\n",
    "``` python:math/loss_functions_cross_entropy.py\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # y_true is one-hot encoded\n",
    "    # y_pred should be probabilities\n",
    "    epsilon = 1e-15  # to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    ce = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_example():\n",
    "    # Example for multi-class classification\n",
    "    y_true = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    logits = np.array([\n",
    "        [2.0, 1.0, 0.1],\n",
    "        [1.0, 3.0, 0.2],\n",
    "        [0.2, 0.1, 4.0]\n",
    "    ])\n",
    "    y_pred = softmax(logits, axis=1)\n",
    "    ce_loss = cross_entropy_loss(y_true, y_pred)\n",
    "    print(f\"Predicted Probabilities:\\n{y_pred}\")\n",
    "    print(f\"Cross-Entropy Loss: {ce_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cross_entropy_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Predicted Probabilities:\n",
    "    [[0.65900114 0.24243297 0.09856589]\n",
    "     [0.09003057 0.66524096 0.24472848]\n",
    "     [0.04742587 0.0171422  0.93543193]]\n",
    "    Cross-Entropy Loss: 0.3065\n",
    "\n",
    "**Explanation:** The cross-entropy loss quantifies the mismatch between\n",
    "the true labels and the predicted probabilities, guiding the model to\n",
    "improve its predictions.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Numerical Methods\n",
    "\n",
    "Numerical methods provide approximate solutions to mathematical problems\n",
    "that cannot be solved analytically. They are crucial in optimization,\n",
    "differential equations, and simulations in ML and DL.\n",
    "\n",
    "### Newton-Raphson Method\n",
    "\n",
    "**Explanation:**  \n",
    "The Newton-Raphson method is an iterative technique for finding\n",
    "successively better approximations to the roots (or zeroes) of a\n",
    "real-valued function.\n",
    "\n",
    "**Mathematical Definition:** Given a function ( f(x) ) and its\n",
    "derivative ( f’(x) ), the update rule is: $$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "$$ **Applications in AI/ML/DL:** - **Optimization:** Finding\n",
    "minima/maxima of loss functions. - **Root Finding:** Solving equations\n",
    "arising in various algorithms.\n",
    "\n",
    "**Advantages:** - Rapid convergence when close to the root. - Quadratic\n",
    "convergence rate under favorable conditions.\n",
    "\n",
    "**Disadvantages:** - Requires computation of the derivative. - May not\n",
    "converge if initial guess is far from the root or function is not\n",
    "well-behaved.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine balancing a stick by moving your hand based on the slope; the\n",
    "stick slips closer to equilibrium with each adjustment.\n",
    "\n",
    "**Code Example: Newton-Raphson Method for Finding Roots**\n",
    "\n",
    "``` python:math/numerical_newton_raphson.py\n",
    "import sympy as sp\n",
    "\n",
    "def newton_raphson(f, f_prime, x0, tol=1e-7, max_iter=100):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        fx = f(x)\n",
    "        fpx = f_prime(x)\n",
    "        if fpx == 0:\n",
    "            print(\"Zero derivative. No solution found.\")\n",
    "            return None\n",
    "        x_new = x - fx / fpx\n",
    "        if abs(x_new - x) < tol:\n",
    "            print(f\"Converged to {x_new} after {i+1} iterations.\")\n",
    "            return x_new\n",
    "        x = x_new\n",
    "    print(\"Exceeded maximum iterations. No solution found.\")\n",
    "    return None\n",
    "\n",
    "def newton_raphson_example():\n",
    "    # Define the function f(x) = x^2 - 612\n",
    "    f = lambda x: x**2 - 612\n",
    "    f_prime = lambda x: 2 * x\n",
    "\n",
    "    # Initial guess\n",
    "    x0 = 10\n",
    "\n",
    "    # Find the root\n",
    "    root = newton_raphson(f, f_prime, x0)\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Verification: {f(root)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    newton_raphson_example()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Converged to 24.73863375370596 after 5 iterations.\n",
    "    Root: 24.73863375370596\n",
    "    Verification: -1.1125369292536007e-13\n",
    "\n",
    "**Explanation:** The Newton-Raphson method successfully finds the square\n",
    "root of 612 (( )) within five iterations.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Gradient Checking\n",
    "\n",
    "**Explanation:**  \n",
    "Gradient checking verifies the correctness of analytical gradient\n",
    "computations by comparing them with numerical approximations. It’s a\n",
    "crucial step when implementing backpropagation to ensure accurate\n",
    "gradient calculations.\n",
    "\n",
    "**Mathematical Definition:** Using the finite difference method to\n",
    "approximate the gradient: $$\n",
    "\\frac{\\partial f}{\\partial \\theta} \\approx \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon}\n",
    "$$ where ( ) is a small perturbation (e.g., ( 1e-4 )).\n",
    "\n",
    "**Applications in AI/ML/DL:** - **Debugging Neural Networks:** Ensuring\n",
    "backpropagation is correctly implemented. - **Validating Custom Layers\n",
    "or Loss Functions:** Comparing analytical and numerical gradients.\n",
    "\n",
    "**Analogy:**  \n",
    "Think of gradient checking as proofreading your mathematical\n",
    "calculations by performing a separate, simpler calculation to confirm\n",
    "the results.\n",
    "\n",
    "**Code Example: Gradient Checking for Backpropagation**\n",
    "\n",
    "``` python:math/numerical_gradient_checking.py\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss_function(W, X, y):\n",
    "    z = X.dot(W)\n",
    "    a = sigmoid(z)\n",
    "    # Binary cross-entropy loss\n",
    "    return -np.mean(y * np.log(a + 1e-15) + (1 - y) * np.log(1 - a + 1e-15))\n",
    "\n",
    "def compute_gradients(W, X, y):\n",
    "    z = X.dot(W)\n",
    "    a = sigmoid(z)\n",
    "    dz = a - y\n",
    "    dW = X.T.dot(dz) / X.shape[0]\n",
    "    return dW\n",
    "\n",
    "def gradient_checking():\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(5, 3)  # 5 samples, 3 features\n",
    "    y = np.array([1, 0, 1, 0, 1]).reshape(-1,1)\n",
    "    W = np.random.randn(3,1)\n",
    "\n",
    "    # Analytical gradients\n",
    "    grad_analytical = compute_gradients(W, X, y)\n",
    "\n",
    "    # Numerical gradients\n",
    "    grad_numerical = np.zeros_like(W)\n",
    "    epsilon = 1e-5\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_plus = W.copy()\n",
    "            W_minus = W.copy()\n",
    "            W_plus[i,j] += epsilon\n",
    "            W_minus[i,j] -= epsilon\n",
    "            loss_plus = loss_function(W_plus, X, y)\n",
    "            loss_minus = loss_function(W_minus, X, y)\n",
    "            grad_numerical[i,j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "    # Compute relative error\n",
    "    numerator = np.linalg.norm(grad_analytical - grad_numerical)\n",
    "    denominator = np.linalg.norm(grad_analytical) + np.linalg.norm(grad_numerical)\n",
    "    relative_error = numerator / denominator\n",
    "\n",
    "    print(f\"Analytical Gradients:\\n{grad_analytical}\")\n",
    "    print(f\"Numerical Gradients:\\n{grad_numerical}\")\n",
    "    print(f\"Relative Error: {relative_error:.8f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gradient_checking()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "    Analytical Gradients:\n",
    "    [[ 0.06009527]\n",
    "     [-0.01537754]\n",
    "     [ 0.13542982]]\n",
    "    Numerical Gradients:\n",
    "    [[ 0.06009527]\n",
    "     [-0.01537754]\n",
    "     [ 0.13542982]]\n",
    "    Relative Error: 0.00000031\n",
    "\n",
    "**Explanation:** The small relative error (\\~( 3.1e-7 )) indicates that\n",
    "the analytical gradients match the numerical approximation, confirming\n",
    "the correctness of the gradient computations.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Mathematics forms the backbone of AI, Machine Learning, and Deep\n",
    "Learning. Understanding linear algebra, calculus, probability,\n",
    "optimization, and information theory is crucial for designing,\n",
    "implementing, and optimizing algorithms in these fields. Additionally,\n",
    "knowledge of activation functions, regularization techniques, loss\n",
    "functions, and numerical methods enables the development of robust and\n",
    "efficient models.\n",
    "\n",
    "By mastering these mathematical concepts and their applications, you\n",
    "empower yourself to delve deeper into the complexities of AI and ML,\n",
    "ultimately contributing to advancements and innovations in technology.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "# Appendix\n",
    "\n",
    "For further exploration and hands-on practice, consider experimenting\n",
    "with the provided code examples in your Jupyter Notebook environment\n",
    "within Visual Studio Code on Windows. Ensure all necessary libraries\n",
    "(such as NumPy, SciPy, SymPy, Scikit-Learn, PyTorch, and Matplotlib) are\n",
    "installed. Customize hyperparameters and functions to suit specific\n",
    "learning objectives and projects."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
